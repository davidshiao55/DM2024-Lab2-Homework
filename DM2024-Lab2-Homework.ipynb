{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tckObZEkV9VH"
   },
   "source": [
    "### Student Information\n",
    "Name: 蕭泓佐\n",
    "\n",
    "Student ID: r13922163\n",
    "\n",
    "GitHub ID: davidshiao55\n",
    "\n",
    "Kaggle name: David Shiao\n",
    "\n",
    "Kaggle private scoreboard snapshot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3XZ2AM3V9VI"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHCChtDXV9VJ"
   },
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvrnRS4BV9VJ"
   },
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook.\n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking:\n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained.\n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji==0.6.0\n",
      "  Downloading emoji-0.6.0.tar.gz (51 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-0.6.0-py3-none-any.whl size=49733 sha256=03ce2781f1226ecad28f41f73f9b0d65980b00eb83cf6b0cf5410794fda7aad8\n",
      "  Stored in directory: /root/.cache/pip/wheels/1b/bd/d9/310c33c45a553798a714e27e3b8395d37128425442b8c78e07\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-0.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install emoji==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'data'\n",
    "\n",
    "# input_path = '/kaggle/input/dm-2024-isa-5810-lab-2-homework'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxGoIMMtfpcG"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BlA6nTfQV9VJ",
    "outputId": "2f29d6de-57c6-43ed-c490-cf5e3e1cd369"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p ~/.kaggle\n",
    "# !cp /DM2024-Lab2-Homework/kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# !kaggle competitions download -c dm-2024-isa-5810-lab-2-homework\n",
    "# !mkdir -p ./data\n",
    "# !unzip -q dm-2024-isa-5810-lab-2-homework.zip -d ./data\n",
    "# !rm dm-2024-isa-5810-lab-2-homework.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "4mkqPsQBWShQ",
    "outputId": "c539e988-acb0-4395-83f6-c591bb7e2594"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id identification\n",
       "0  0x28cc61           test\n",
       "1  0x29e452          train\n",
       "2  0x2b3819          train\n",
       "3  0x2db41f           test\n",
       "4  0x2a2acc          train"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_ids = pd.read_csv(f'{input_path}/data_identification.csv')\n",
    "data_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "2z5V1Xy5XdQK",
    "outputId": "8bf5177d-6aad-4e6f-c689-febe2b8f5f8b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x2a8830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x20b21d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id\n",
       "1  0x29e452\n",
       "2  0x2b3819\n",
       "4  0x2a2acc\n",
       "5  0x2a8830\n",
       "6  0x20b21d"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_ids = data_ids[data_ids['identification'] == 'train'].drop(['identification'], axis=1)\n",
    "test_data_ids = data_ids[data_ids['identification'] == 'test'].drop(['identification'], axis=1)\n",
    "\n",
    "train_data_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "n8DfUCWqWuQi",
    "outputId": "925e477e-c609-42db-a0a0-5488f5cd9d89"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x3140b1</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x368b73</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x296183</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2bd6e1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2ee1dd</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       emotion\n",
       "0  0x3140b1       sadness\n",
       "1  0x368b73       disgust\n",
       "2  0x296183  anticipation\n",
       "3  0x2bd6e1           joy\n",
       "4  0x2ee1dd  anticipation"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_labels = pd.read_csv(f'{input_path}/emotion.csv')\n",
    "emotion_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "e6ik_1LcW_oQ",
    "outputId": "e099647f-ba78-4c96-c52e-2f5ca7d6da98"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "      <th>tweet.hashtags</th>\n",
       "      <th>tweet.tweet_id</th>\n",
       "      <th>tweet.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>989</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _score          _index           _crawldate   _type  \\\n",
       "0     391  hashtag_tweets  2015-05-23 11:42:47  tweets   \n",
       "1     433  hashtag_tweets  2016-01-28 04:52:09  tweets   \n",
       "2     232  hashtag_tweets  2017-12-25 04:39:20  tweets   \n",
       "3     376  hashtag_tweets  2016-01-24 23:53:05  tweets   \n",
       "4     989  hashtag_tweets  2016-01-08 17:18:59  tweets   \n",
       "\n",
       "                  tweet.hashtags tweet.tweet_id  \\\n",
       "0                     [Snapchat]       0x376b20   \n",
       "1  [freepress, TrumpLegacy, CNN]       0x2d5350   \n",
       "2                   [bibleverse]       0x28b412   \n",
       "3                             []       0x1cd5b0   \n",
       "4                             []       0x2de201   \n",
       "\n",
       "                                          tweet.text  \n",
       "0  People who post \"add me on #Snapchat\" must be ...  \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  \n",
       "2  Confident of your obedience, I write to you, k...  \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>  \n",
       "4  \"Trust is not the same as faith. A friend is s...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_json(f'{input_path}/tweets_DM.json', lines=True)\n",
    "\n",
    "# Use json_normalize to flatten only the `_source` column and keep other columns intact\n",
    "source_df = pd.json_normalize(tweets_df['_source'])\n",
    "\n",
    "# Concatenate the flattened `_source` columns with the original dataframe\n",
    "tweets_df = pd.concat([tweets_df.drop(columns=['_source']), source_df], axis=1)\n",
    "\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "LSToyFxIW2io",
    "outputId": "a0fb7772-3f8d-4217-b934-8929642589e9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "      <th>tweet.hashtags</th>\n",
       "      <th>tweet.tweet_id</th>\n",
       "      <th>tweet.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>joy</td>\n",
       "      <td>809</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2015-01-17 03:07:03</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x29e452</td>\n",
       "      <td>Huge Respect🖒 @JohnnyVegasReal talking about l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>joy</td>\n",
       "      <td>808</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-07-02 09:34:06</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[spateradio, app]</td>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>Yoooo we hit all our monthly goals with the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>trust</td>\n",
       "      <td>16</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-08-15 18:18:39</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>@KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2a8830</td>\n",
       "      <td>joy</td>\n",
       "      <td>768</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2017-02-11 08:49:46</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[PUBG, GamersUnite, twitch, BeHealthy, StayPos...</td>\n",
       "      <td>0x2a8830</td>\n",
       "      <td>Come join @ambushman27 on #PUBG while he striv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x20b21d</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>70</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-11-23 05:37:10</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[strength, bones, God]</td>\n",
       "      <td>0x20b21d</td>\n",
       "      <td>@fanshixieen2014 Blessings!My #strength little...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       emotion  _score          _index           _crawldate  \\\n",
       "0  0x29e452           joy     809  hashtag_tweets  2015-01-17 03:07:03   \n",
       "1  0x2b3819           joy     808  hashtag_tweets  2016-07-02 09:34:06   \n",
       "2  0x2a2acc         trust      16  hashtag_tweets  2016-08-15 18:18:39   \n",
       "3  0x2a8830           joy     768  hashtag_tweets  2017-02-11 08:49:46   \n",
       "4  0x20b21d  anticipation      70  hashtag_tweets  2016-11-23 05:37:10   \n",
       "\n",
       "    _type                                     tweet.hashtags tweet.tweet_id  \\\n",
       "0  tweets                                                 []       0x29e452   \n",
       "1  tweets                                  [spateradio, app]       0x2b3819   \n",
       "2  tweets                                                 []       0x2a2acc   \n",
       "3  tweets  [PUBG, GamersUnite, twitch, BeHealthy, StayPos...       0x2a8830   \n",
       "4  tweets                             [strength, bones, God]       0x20b21d   \n",
       "\n",
       "                                          tweet.text  \n",
       "0  Huge Respect🖒 @JohnnyVegasReal talking about l...  \n",
       "1  Yoooo we hit all our monthly goals with the ne...  \n",
       "2  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...  \n",
       "3  Come join @ambushman27 on #PUBG while he striv...  \n",
       "4  @fanshixieen2014 Blessings!My #strength little...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.merge(train_data_ids, emotion_labels, on='tweet_id', how='inner')\n",
    "train_data = pd.merge(train_data, tweets_df, left_on='tweet_id', right_on='tweet.tweet_id', how='inner')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "tQ3sHUB9cV-N",
    "outputId": "013ea68b-0132-4719-d017-808fc1d822fe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "      <th>tweet.hashtags</th>\n",
       "      <th>tweet.tweet_id</th>\n",
       "      <th>tweet.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>107</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2017-01-17 14:13:32</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>@Habbo I've seen two separate colours of the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>728</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2015-10-17 06:46:20</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>@FoxNews @KellyannePolls No serious self respe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2466f6</td>\n",
       "      <td>491</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-12-19 03:50:27</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[womendrivers]</td>\n",
       "      <td>0x2466f6</td>\n",
       "      <td>Looking for a new car, and it says 1 lady owne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x23f9e9</td>\n",
       "      <td>28</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2017-04-09 19:32:19</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[robbingmembers]</td>\n",
       "      <td>0x23f9e9</td>\n",
       "      <td>@cineworld “only the brave” just out and fount...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1fb4e1</td>\n",
       "      <td>925</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>2016-01-15 11:59:31</td>\n",
       "      <td>tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x1fb4e1</td>\n",
       "      <td>Felt like total dog 💩 going into open gym and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id  _score          _index           _crawldate   _type  \\\n",
       "0  0x28cc61     107  hashtag_tweets  2017-01-17 14:13:32  tweets   \n",
       "1  0x2db41f     728  hashtag_tweets  2015-10-17 06:46:20  tweets   \n",
       "2  0x2466f6     491  hashtag_tweets  2016-12-19 03:50:27  tweets   \n",
       "3  0x23f9e9      28  hashtag_tweets  2017-04-09 19:32:19  tweets   \n",
       "4  0x1fb4e1     925  hashtag_tweets  2016-01-15 11:59:31  tweets   \n",
       "\n",
       "     tweet.hashtags tweet.tweet_id  \\\n",
       "0                []       0x28cc61   \n",
       "1                []       0x2db41f   \n",
       "2    [womendrivers]       0x2466f6   \n",
       "3  [robbingmembers]       0x23f9e9   \n",
       "4                []       0x1fb4e1   \n",
       "\n",
       "                                          tweet.text  \n",
       "0  @Habbo I've seen two separate colours of the e...  \n",
       "1  @FoxNews @KellyannePolls No serious self respe...  \n",
       "2  Looking for a new car, and it says 1 lady owne...  \n",
       "3  @cineworld “only the brave” just out and fount...  \n",
       "4  Felt like total dog 💩 going into open gym and ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.merge(test_data_ids, tweets_df, left_on='tweet_id', right_on='tweet.tweet_id', how='inner')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "joy             516017\n",
       "anticipation    248935\n",
       "trust           205478\n",
       "sadness         193437\n",
       "disgust         139101\n",
       "fear             63999\n",
       "surprise         48729\n",
       "anger            39867\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyze class distribution\n",
    "train_data['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A9UDP_AfodN"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emoji import demojize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def normalizeToken(token):\n",
    "    lowercased_token = token.lower()\n",
    "    if token.startswith(\"@\"):\n",
    "        return \"@USER\"\n",
    "    elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
    "        return \"HTTPURL\"\n",
    "    elif len(token) == 1:\n",
    "        return demojize(token)\n",
    "    else:\n",
    "        if token == \"’\":\n",
    "            return \"'\"\n",
    "        elif token == \"…\":\n",
    "            return \"...\"\n",
    "        else:\n",
    "            return token\n",
    "\n",
    "\n",
    "def normalizeTweet(tweet):\n",
    "    tokens = tokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
    "    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
    "\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"cannot \", \"can not \")\n",
    "        .replace(\"n't \", \" n't \")\n",
    "        .replace(\"n 't \", \" n't \")\n",
    "        .replace(\"ca n't\", \"can't\")\n",
    "        .replace(\"ai n't\", \"ain't\")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"'m \", \" 'm \")\n",
    "        .replace(\"'re \", \" 're \")\n",
    "        .replace(\"'s \", \" 's \")\n",
    "        .replace(\"'ll \", \" 'll \")\n",
    "        .replace(\"'d \", \" 'd \")\n",
    "        .replace(\"'ve \", \" 've \")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\" p . m .\", \"  p.m.\")\n",
    "        .replace(\" p . m \", \" p.m \")\n",
    "        .replace(\" a . m .\", \" a.m.\")\n",
    "        .replace(\" a . m \", \" a.m \")\n",
    "    )\n",
    "\n",
    "    return \" \".join(normTweet.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove <LH> tags\n",
    "    text = text.replace('<LH>', '')\n",
    "    \n",
    "    # normalize tweet\n",
    "    text = normalizeTweet(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "emotion_mapping = {\n",
    "    \"joy\": 0,\n",
    "    \"anticipation\": 1,\n",
    "    \"trust\": 2,\n",
    "    \"sadness\": 3,\n",
    "    \"disgust\": 4,\n",
    "    \"fear\": 5,\n",
    "    \"surprise\": 6,\n",
    "    \"anger\": 7,\n",
    "}\n",
    "\n",
    "# Reverse mapping for decoding\n",
    "reverse_emotion_mapping = {v: k for k, v in emotion_mapping.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing function to the text column\n",
    "train_data['processed_text'] = train_data['tweet.text'].apply(preprocess_text)\n",
    "train_data['emotion_label'] = train_data[\"emotion\"].map(emotion_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ed_hrEVqin1i"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_data['processed_text']\n",
    "y = train_data['emotion_label']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFO4QY2ufzzZ"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(X_train, X_val, tokenizer, max_length=128):\n",
    "    # Tokenize text data\n",
    "    train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    val_encodings = tokenizer(list(X_val), truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "    return train_encodings, val_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "loUP-827g_rt"
   },
   "outputs": [],
   "source": [
    "# Dataset class for loading text data from DataFrame\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings  # Should already contain tensors if return_tensors=\"pt\" was used\n",
    "        self.labels = torch.tensor(labels)  # Ensure labels are converted to a tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Use encodings as they are without re-wrapping in torch.tensor\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_model(model, optimizer, scheduler, train_dataloader, val_dataloader, device, num_epochs, save_dir=\"./models\"):    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Training phase\n",
    "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_attention_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=b_input_ids, attention_mask=b_attention_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip the gradient to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        # Step the scheduler after each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=\"Validating\"):\n",
    "                b_input_ids = batch['input_ids'].to(device)\n",
    "                b_attention_mask = batch['attention_mask'].to(device)\n",
    "                b_labels = batch['labels'].to(device)\n",
    "                \n",
    "                # Forward pass for validation\n",
    "                outputs = model(input_ids=b_input_ids, attention_mask=b_attention_mask, labels=b_labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                # Store predictions and true labels\n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(b_labels.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        \n",
    "        # Decode numerical labels to original class names\n",
    "        decoded_preds = [reverse_emotion_mapping[pred] for pred in all_preds]\n",
    "        decoded_labels = [reverse_emotion_mapping[label] for label in all_labels]\n",
    "        \n",
    "        # Generate classification report\n",
    "        class_names = list(emotion_mapping.keys())\n",
    "        class_report = classification_report(decoded_labels, decoded_preds, digits=4, target_names=class_names)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(class_report)\n",
    "\n",
    "        # Save model checkpoint\n",
    "        save_path = f\"{save_dir}/ep_{epoch + 1}\"\n",
    "        model.save_pretrained(save_path)\n",
    "        print(f\"Model checkpoint saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"vinai/bertweet-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoding, X_val_encoding = encode_data(X_train, X_val, tokenizer, max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize datasets\n",
    "train_dataset = TweetDataset(X_train_encoding, y_train.tolist())\n",
    "val_dataset = TweetDataset(X_val_encoding, y_val.tolist())\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.7)\n",
    "train_model(model, optimizer, scheduler, train_dataloader, val_dataloader, device, num_epochs=3, save_dir=\"./models/Bertweet_large_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['processed_text'] = test_data['tweet.text'].apply(preprocess_text)\n",
    "X_test = test_data['processed_text']\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"vinai/bertweet-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, normalization=True)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=48, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for batching\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    test_encodings[\"input_ids\"],\n",
    "    test_encodings[\"attention_mask\"]\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=512)\n",
    "\n",
    "model_path = \"./models/Bertweet_large_v1/ep_3\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   7%|▋         | 59/805 [03:45<47:26,  3.82s/it]"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Perform predictions\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "\n",
    "        # Get logits from the model\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Get the predicted labels\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Add predictions to the test DataFrame\n",
    "test_data['emotion_label'] = predictions\n",
    "test_data['emotion'] = test_data['emotion_label'].map(reverse_emotion_mapping)\n",
    "\n",
    "# Keep only the desired columns\n",
    "submission = test_data[['tweet_id', 'emotion']]\n",
    "\n",
    "# Rename the 'tweet_id' column to 'id'\n",
    "submission = submission.rename(columns={'tweet_id': 'id'})\n",
    "\n",
    "# Save or display the predictions\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the proper sequence length for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token length: 882\n",
      "95th percentile token length: 47.0\n",
      "Average token length: 26.313791982895964\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"vinai/bertweet-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize without truncation\n",
    "token_lengths = [len(tokenizer.encode(text, truncation=False)) for text in train_data['processed_text']]\n",
    "\n",
    "print(f\"Max token length: {max(token_lengths)}\")\n",
    "print(f\"95th percentile token length: {np.percentile(token_lengths, 95)}\")\n",
    "print(f\"Average token length: {np.mean(token_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze for unrecognized tokens in the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown tokens and tokenized results have been saved to unknown_tokens.json.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "model_name = \"vinai/bertweet-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Access the tokenizer vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Identify unknown tokens in training data\n",
    "unknown_tokens_analysis = []\n",
    "\n",
    "for text in train_data['processed_text']:\n",
    "    tokens = tokenizer.tokenize(text)  # Tokenize the text\n",
    "    unk_tokens = [token for token in tokens if token not in vocab]\n",
    "    if unk_tokens:\n",
    "        unknown_tokens_analysis.append({\n",
    "            \"text\": text,\n",
    "            \"tokenized\": tokens,\n",
    "            \"unknown_tokens\": unk_tokens\n",
    "        })\n",
    "\n",
    "# Save results to a file\n",
    "output_file = \"unknown_tokens.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unknown_tokens_analysis, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Unknown tokens and tokenized results have been saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = \"vinai/bertweet-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Congratulations to all our #Graduates from @StOswaldsPriLiv   From <LH> #learningisfun <LH> #primary #Liverpool #aspire <LH>'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 14\n",
    "train_data['tweet.text'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Congratulations to all our #Graduates from @USER From #learningisfun #primary #Liverpool #aspire'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['processed_text'][i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'Congratulations',\n",
       " 'Ġto',\n",
       " 'Ġall',\n",
       " 'Ġour',\n",
       " 'Ġ#',\n",
       " 'Grad',\n",
       " 'uates',\n",
       " 'Ġfrom',\n",
       " 'Ġ@',\n",
       " 'USER',\n",
       " 'ĠFrom',\n",
       " 'Ġ#',\n",
       " 'learning',\n",
       " 'isf',\n",
       " 'un',\n",
       " 'Ġ#',\n",
       " 'primary',\n",
       " 'Ġ#',\n",
       " 'Liverpool',\n",
       " 'Ġ#',\n",
       " 'as',\n",
       " 'pire']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(train_data['processed_text'][i])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
